import re
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
from stacks.downloader.sites.zlib import parse_zlib_download_link, is_zlib_domain
from stacks.constants import LEGAL_FILES

def parse_download_link_from_html(d, html_content, md5, mirror_url=None):
        """
        ä» HTML å†…å®¹ä¸­è§£æå‡ºå®é™…çš„æ–‡ä»¶ä¸‹è½½é“¾æ¥
        
        è¿™ä¸ªå‡½æ•°æ˜¯"é“¾æ¥æå–å™¨"ï¼Œå®ƒçš„ä»»åŠ¡æ˜¯ä»é•œåƒç«™ç‚¹çš„ HTML é¡µé¢ä¸­
        æ‰¾åˆ°çœŸæ­£çš„æ–‡ä»¶ä¸‹è½½é“¾æ¥ã€‚ç”±äºä¸åŒçš„é•œåƒç«™ç‚¹ç»“æ„ä¸åŒï¼Œæˆ‘ä»¬éœ€è¦
        ä½¿ç”¨å¤šç§æ–¹æ³•æ¥æå–é“¾æ¥ã€‚
        
        æ ¸å¿ƒæŒ‘æˆ˜ï¼š
        1. ä¸åŒé•œåƒç«™ç‚¹çš„ HTML ç»“æ„ä¸åŒ
        2. ä¸‹è½½é“¾æ¥å¯èƒ½éšè—åœ¨å„ç§ HTML å…ƒç´ ä¸­ï¼ˆ<a>ã€<button>ã€<span> ç­‰ï¼‰
        3. éœ€è¦åŒºåˆ†çœŸæ­£çš„ä¸‹è½½é“¾æ¥å’Œå¯¼èˆªé“¾æ¥
        
        Args:
            d: ä¸‹è½½å™¨å®ä¾‹ï¼ˆç”¨äºè®¿é—®æ—¥å¿—è®°å½•å™¨ç­‰ï¼‰
            html_content: ä»é•œåƒç«™ç‚¹è·å–çš„ HTML å†…å®¹
            md5: æ–‡ä»¶çš„ MD5 å“ˆå¸Œå€¼ï¼ˆç”¨äºéªŒè¯é“¾æ¥çš„æ­£ç¡®æ€§ï¼‰
            mirror_url: é•œåƒç«™ç‚¹çš„ URLï¼ˆç”¨äºç«™ç‚¹ç‰¹å®šçš„è§£æå™¨ï¼‰
        
        Returns:
            str: ä¸‹è½½é“¾æ¥çš„å®Œæ•´ URL
            None: å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸‹è½½é“¾æ¥
        
        å·¥ä½œæµç¨‹ï¼š
        1. é¦–å…ˆå°è¯•ä½¿ç”¨ç«™ç‚¹ç‰¹å®šçš„è§£æå™¨ï¼ˆå¦‚ Z-Libraryï¼‰
        2. å¦‚æœç«™ç‚¹ç‰¹å®šè§£æå™¨å¤±è´¥ï¼Œä½¿ç”¨é€šç”¨è§£ææ–¹æ³•
        3. é€šç”¨è§£ææ–¹æ³•ä¼šå°è¯• 4 ç§ä¸åŒçš„ç­–ç•¥æ¥æŸ¥æ‰¾ä¸‹è½½é“¾æ¥
        """
        # ==================== æ­¥éª¤ 1: å°è¯•ä½¿ç”¨ç«™ç‚¹ç‰¹å®šçš„è§£æå™¨ ====================
        if mirror_url:
            # æ£€æŸ¥æ˜¯å¦æ˜¯ Z-Library åŸŸå
            if is_zlib_domain(mirror_url):
                d.logger.debug("ä½¿ç”¨ Z-Library ä¸“ç”¨è§£æå™¨")
                # è°ƒç”¨ Z-Library ä¸“ç”¨çš„é“¾æ¥è§£æå‡½æ•°
                download_link = parse_zlib_download_link(d, html_content, mirror_url)
                if download_link:
                    return download_link
                d.logger.debug("Z-Library è§£æå™¨æœªæ‰¾åˆ°é“¾æ¥ï¼Œå›é€€åˆ°é€šç”¨è§£æå™¨")

        # ==================== æ­¥éª¤ 2: ä½¿ç”¨é€šç”¨è§£ææ–¹æ³• ====================
        # åˆ›å»º BeautifulSoup å¯¹è±¡ï¼Œç”¨äºè§£æ HTML
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # è·å– MD5 çš„å‰ 12 ä¸ªå­—ç¬¦
        # ä¸‹è½½é“¾æ¥é€šå¸¸åŒ…å« MD5 çš„å‰ç¼€ä½œä¸ºæ–‡ä»¶åçš„ä¸€éƒ¨åˆ†
        # ä¾‹å¦‚ï¼šhttps://example.com/files/abc123def456.../book.epub
        md5_prefix = md5[:12]
        
        # å®šä¹‰éœ€è¦è·³è¿‡çš„åŸŸååˆ—è¡¨
        # è¿™äº›åŸŸåä¸æ˜¯çœŸæ­£çš„æ–‡ä»¶æ‰˜ç®¡ç«™ç‚¹ï¼Œè€Œæ˜¯å¯¼èˆªé“¾æ¥æˆ–ç¤¾äº¤åª’ä½“é“¾æ¥
        skip_domains = [
            'jdownloader.org',      # ä¸‹è½½ç®¡ç†å™¨ç½‘ç«™
            'telegram.org', 't.me', # Telegram é“¾æ¥
            'discord.gg',           # Discord é“¾æ¥
            'reddit.com',           # Reddit é“¾æ¥
            'twitter.com',          # Twitter é“¾æ¥
            'facebook.com',         # Facebook é“¾æ¥
            'instagram.com',        # Instagram é“¾æ¥
            'patreon.com',          # Patreon é“¾æ¥
            'ko-fi.com',            # Ko-fi é“¾æ¥
            'buymeacoffee.com',     # æèµ é“¾æ¥
            'annas-archive.org/account',  # è´¦æˆ·é¡µé¢
            'annas-archive.org/search',   # æœç´¢é¡µé¢
            'annas-archive.org/md5',      # MD5 é¡µé¢
            'annas-archive.org/donate',   # æèµ é¡µé¢
            '.onion'                # Tor æš—ç½‘é“¾æ¥
        ]

        # ==================== æ–¹æ³• 1: æŸ¥æ‰¾åŒ…å« MD5 å‰ç¼€çš„é“¾æ¥ ====================
        # è¿™æ˜¯æœ€å¸¸ç”¨çš„æ–¹æ³•ï¼Œé€‚ç”¨äº slow_download é¡µé¢
        # ä¸‹è½½é“¾æ¥é€šå¸¸åŒ…å«æ–‡ä»¶çš„ MD5 å‰ç¼€
        for link in soup.find_all('a', href=True):
            href = link['href']

            # å¿…é¡»æ˜¯å®Œæ•´çš„ URLï¼ˆä»¥ http:// æˆ– https:// å¼€å¤´ï¼‰
            if not href.startswith('http'):
                continue

            # è·³è¿‡å¯¼èˆªé“¾æ¥ã€ç¤¾äº¤åª’ä½“é“¾æ¥å’Œ .onion é“¾æ¥
            if any(skip in href.lower() for skip in skip_domains):
                continue

            # è·³è¿‡ slow_download é¡µé¢æœ¬èº«
            # æˆ‘ä»¬è¦æ‰¾çš„æ˜¯çœŸæ­£çš„æ–‡ä»¶ä¸‹è½½é“¾æ¥ï¼Œè€Œä¸æ˜¯å¦ä¸€ä¸ª slow_download é¡µé¢
            if 'slow_download' in href.lower():
                continue

            # å¦‚æœé“¾æ¥åŒ…å« MD5 å‰ç¼€ï¼Œå¾ˆå¯èƒ½æ˜¯ä¸‹è½½é“¾æ¥
            if md5_prefix in href.lower():
                d.logger.debug(f"æ‰¾åˆ°åŒ…å« MD5 å‰ç¼€çš„ä¸‹è½½é“¾æ¥: {href}")
                return href
        
        # ==================== æ–¹æ³• 2: æŸ¥æ‰¾åŒ…å«æ–‡ä»¶æ‰©å±•åçš„é“¾æ¥ ====================
        # è¿™æ˜¯å¤–éƒ¨é•œåƒçš„å¤‡ç”¨æ–¹æ³•
        # æŸ¥æ‰¾åŒ…å« "download" æˆ– "get" æ–‡æœ¬çš„é“¾æ¥ï¼Œå¹¶ä¸”é“¾æ¥åŒ…å«åˆæ³•çš„æ–‡ä»¶æ‰©å±•å
        for link in soup.find_all('a', href=True):
            href = link['href']
            link_text = link.get_text().strip().lower()
            
            # å¿…é¡»æ˜¯å®Œæ•´çš„ URL
            if not href.startswith('http'):
                continue
            
            # è·³è¿‡å¯¼èˆªé“¾æ¥
            if any(skip in href.lower() for skip in skip_domains):
                continue
            
            # æŸ¥æ‰¾åŒ…å«ä¸‹è½½æŒ‡ç¤ºè¯çš„é“¾æ¥
            if 'download' in link_text or 'get' in link_text:
                # æ£€æŸ¥é“¾æ¥æ˜¯å¦åŒ…å«åˆæ³•çš„æ–‡ä»¶æ‰©å±•åæˆ–å¸¸è§çš„ä¸‹è½½è„šæœ¬
                if any(ext in href.lower() for ext in LEGAL_FILES) \
                   or 'get.php' in href.lower() or 'main.php' in href.lower():
                    d.logger.debug(f"é€šè¿‡å¤‡ç”¨æ–¹æ³•æ‰¾åˆ°ä¸‹è½½é“¾æ¥: {href}")
                    return href

        # ==================== æ–¹æ³• 3: æŸ¥æ‰¾åŒ…å« URL çš„å‰ªè´´æ¿æŒ‰é’® ====================
        # æœ‰äº›ç«™ç‚¹ä½¿ç”¨ JavaScript å°†ä¸‹è½½é“¾æ¥å¤åˆ¶åˆ°å‰ªè´´æ¿
        # æŒ‰é’®çš„ onclick äº‹ä»¶ä¸­åŒ…å« writeText() å‡½æ•°ï¼Œå‚æ•°å°±æ˜¯ä¸‹è½½é“¾æ¥
        for btn in soup.find_all('button', onclick=True):
            onclick = btn['onclick']
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå– writeText() å‡½æ•°ä¸­çš„ URL
            match = re.search(r"writeText\('([^']+)'", onclick)
            if match:
                url = match.group(1)

                # éªŒè¯ URL æ˜¯å¦åŒ…å« MD5 å‰ç¼€
                if md5_prefix not in url:
                    continue

                # è¿”å›å®Œæ•´çš„ URLï¼ˆåŒ…æ‹¬ç­¾åéƒ¨åˆ†ï¼Œå³ ~/ ä¹‹åçš„æ‰€æœ‰å†…å®¹ï¼‰
                d.logger.debug(f"ä»å‰ªè´´æ¿æŒ‰é’®æ‰¾åˆ° URL: {url}")
                return url
            
        # ==================== æ–¹æ³• 4: æŸ¥æ‰¾åŒ…å«åŸå§‹ URL çš„ span å…ƒç´  ====================
        # æœ‰äº›ç«™ç‚¹ç›´æ¥åœ¨ <span> æ ‡ç­¾ä¸­æ˜¾ç¤ºä¸‹è½½é“¾æ¥
        for span in soup.find_all('span'):
            text = span.get_text(strip=True)

            # å¿…é¡»ä»¥ http å¼€å¤´
            if not text.startswith("http"):
                continue
            # å¿…é¡»åŒ…å« MD5 å‰ç¼€
            if md5_prefix not in text:
                continue

            # è¿”å›å®Œæ•´çš„ URL
            d.logger.debug(f"ä» span å…ƒç´ æ‰¾åˆ°åŸå§‹ URL: {text}")
            return text

        # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥äº†ï¼Œè¿”å› None
        return None
    
def get_download_links(d, md5):
    """
    ä» Anna's Archive ç½‘ç«™è·å–ä¹¦ç±çš„ä¸‹è½½é“¾æ¥åˆ—è¡¨å’Œæ–‡ä»¶å
    
    è¿™ä¸ªå‡½æ•°æ˜¯"é“¾æ¥æ”¶é›†å™¨"ï¼Œå®ƒçš„ä»»åŠ¡æ˜¯ï¼š
    1. è®¿é—® Anna's Archive çš„ä¹¦ç±é¡µé¢
    2. ä»é¡µé¢ä¸­æå–æ–‡ä»¶åï¼ˆç”¨äºä¿å­˜ä¸‹è½½çš„æ–‡ä»¶ï¼‰
    3. ä»é¡µé¢ä¸­æå–æ‰€æœ‰å¯ç”¨çš„ä¸‹è½½é“¾æ¥ï¼ˆåŒ…æ‹¬æ…¢é€Ÿä¸‹è½½å’Œå¤–éƒ¨é•œåƒï¼‰
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    - æ™ºèƒ½æ–‡ä»¶åæå–ï¼ˆæ”¯æŒå¤šç§æå–ç­–ç•¥ï¼‰
    - è¿‡æ»¤æ‰ç­‰å¾…é˜Ÿåˆ—æœåŠ¡å™¨ï¼ˆåªé€‰æ‹©æ— ç­‰å¾…çš„æœåŠ¡å™¨ï¼‰
    - æ”¶é›†æ‰€æœ‰å¯ç”¨çš„å¤–éƒ¨é•œåƒé“¾æ¥
    
    Args:
        d: ä¸‹è½½å™¨å®ä¾‹ï¼ˆåŒ…å« sessionã€loggerã€é…ç½®ç­‰ï¼‰
        md5: ä¹¦ç±çš„ MD5 å“ˆå¸Œå€¼
    
    Returns:
        tuple: (filename, links)
            - filename: æå–çš„æ–‡ä»¶åï¼ˆå¦‚ "Book Title.epub"ï¼‰
            - links: ä¸‹è½½é“¾æ¥åˆ—è¡¨ï¼Œæ¯ä¸ªé“¾æ¥æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«ï¼š
                - url: å®Œæ•´çš„ä¸‹è½½ URL
                - domain: åŸŸå
                - text: æ˜¾ç¤ºæ–‡æœ¬ï¼ˆæœåŠ¡å™¨åç§°ï¼‰
                - type: é“¾æ¥ç±»å‹ï¼ˆ'slow_download' æˆ– 'external_mirror'ï¼‰
    
    è¿”å›ç¤ºä¾‹ï¼š
        ("Pythonç¼–ç¨‹ä»å…¥é—¨åˆ°å®è·µ.epub", [
            {
                'url': 'https://annas-archive.org/slow_download/abc123...',
                'domain': 'annas-archive.org',
                'text': 'Slow Partner Server',
                'type': 'slow_download'
            },
            {
                'url': 'https://libgen.is/get.php?md5=abc123...',
                'domain': 'libgen.is',
                'text': 'libgen.is',
                'type': 'external_mirror'
            }
        ])
    """
    # æ„å»ºä¹¦ç±é¡µé¢çš„ URL
    # Anna's Archive çš„ä¹¦ç±é¡µé¢æ ¼å¼ï¼šhttps://annas-archive.org/md5/{md5}
    url = f"https://annas-archive.org/md5/{md5}"

    try:
        # ä½¿ç”¨ HTTP ä¼šè¯è®¿é—®ä¹¦ç±é¡µé¢
        # ä½¿ç”¨ session å¯ä»¥ä¿æŒ Cookieï¼Œæé«˜è®¿é—®æˆåŠŸç‡
        response = d.session.get(url, timeout=30)
        response.raise_for_status()  # å¦‚æœè¯·æ±‚å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸

        # ä½¿ç”¨ BeautifulSoup è§£æ HTML å†…å®¹
        soup = BeautifulSoup(response.text, 'html.parser')

        # ==================== è¾…åŠ©å‡½æ•° 1: ä» Filepath å…ƒæ•°æ®ä¸­æå–æ–‡ä»¶å ====================
        def extract_from_filepath():
            """
            ä»é¡µé¢çš„ Filepath å…ƒæ•°æ®ä¸­æå–åŸå§‹æ–‡ä»¶å
            
            Anna's Archive ä¼šæ˜¾ç¤ºæ–‡ä»¶çš„åŸå§‹è·¯å¾„ï¼Œä¾‹å¦‚ï¼š
            - Windows è·¯å¾„ï¼šR:\Books\Pythonç¼–ç¨‹ä»å…¥é—¨åˆ°å®è·µ.epub
            - Unix è·¯å¾„ï¼šlgli/Pythonç¼–ç¨‹ä»å…¥é—¨åˆ°å®è·µ.epub
            
            Returns:
                str: æå–çš„æ–‡ä»¶åï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å› None
            """
            # æŸ¥æ‰¾æ‰€æœ‰åŒ…å« MD5 ä»£ç çš„æ ‡ç­¾
            filepath_elements = soup.find_all('a', class_='js-md5-codes-tabs-tab')
            for element in filepath_elements:
                # æŸ¥æ‰¾æ ‡ç­¾ä¸º "Filepath" çš„ span å…ƒç´ 
                label_span = element.find('span', class_='bg-[#aaa]')
                if label_span and 'Filepath' in label_span.get_text():
                    # è·å–ç¬¬äºŒä¸ª span å…ƒç´ ï¼Œå®ƒåŒ…å«å®é™…çš„æ–‡ä»¶è·¯å¾„
                    filepath_span = element.find_all('span')[1] if len(element.find_all('span')) > 1 else None
                    if filepath_span:
                        filepath_text = filepath_span.get_text().strip()

                        # å¤„ç† Windows é£æ ¼çš„è·¯å¾„ï¼ˆR:\...\filenameï¼‰
                        if '\\' in filepath_text:
                            filename = filepath_text.split('\\')[-1]
                        # å¤„ç† Unix é£æ ¼çš„è·¯å¾„ï¼ˆlgli/filename æˆ– lgrsfic/filenameï¼‰
                        elif '/' in filepath_text:
                            filename = filepath_text.split('/')[-1]
                        else:
                            filename = filepath_text

                        # URL è§£ç æ–‡ä»¶åï¼ˆå°† + æ›¿æ¢ä¸ºç©ºæ ¼ç­‰ï¼‰
                        filename = filename.replace('+', ' ')

                        # å¦‚æœæ‰¾åˆ°äº†æœ‰æ•ˆçš„æ–‡ä»¶åï¼Œè¿”å›å®ƒ
                        if filename and filename.strip():
                            d.logger.info(f"ä» Filepath å…ƒæ•°æ®ä¸­æå–æ–‡ä»¶å: {filename}")
                            return filename
            return None

        # ==================== è¾…åŠ©å‡½æ•° 2: ä»é¡µé¢æ ‡é¢˜ä¸­æå–æ–‡ä»¶å ====================
        def extract_from_title():
            """
            ä»ä¹¦ç±æ ‡é¢˜å’Œå…ƒæ•°æ®ä¸­æ„å»ºæ–‡ä»¶å
            
            è¿™ä¸ªæ–¹æ³•ä¼šï¼š
            1. ä»é¡µé¢æ ‡é¢˜ä¸­æå–ä¹¦ç±åç§°
            2. ä»å…ƒæ•°æ®ä¸­æå–æ–‡ä»¶æ‰©å±•åï¼ˆå¦‚ .pdfã€.epubï¼‰
            3. å°†å®ƒä»¬ç»„åˆæˆå®Œæ•´çš„æ–‡ä»¶å
            
            Returns:
                str: æ„å»ºçš„æ–‡ä»¶åï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å› None
            """
            # æŸ¥æ‰¾ä¹¦ç±ä¿¡æ¯ divï¼ˆåŒ…å«æ ‡é¢˜ï¼‰
            # ä½¿ç”¨ CSS ç±»é€‰æ‹©å™¨æŸ¥æ‰¾æ ‡é¢˜å…ƒç´ 
            title_div = soup.find('div', class_=lambda x: x and 'font-semibold' in x and 'text-2xl' in x and 'leading-[1.2]' in x)
            title = None
            extension = None

            if title_div:
                # è·å–æ–‡æœ¬å†…å®¹ï¼Œæ’é™¤åµŒå¥—æ ‡ç­¾ï¼ˆå¦‚æœç´¢å›¾æ ‡é“¾æ¥ï¼‰
                title = title_div.get_text(strip=True)
                # ç§»é™¤æœç´¢è¡¨æƒ…ç¬¦å·ï¼ˆå¦‚æœæœ‰ï¼‰
                title = title.replace('ğŸ”', '').strip()
                d.logger.info(f"ä»ä¹¦ç±ä¿¡æ¯ div ä¸­æå–æ ‡é¢˜: {title}")
            else:
                d.logger.warning("æœªæ‰¾åˆ°åŒ…å«æ‰€éœ€ç±»çš„æ ‡é¢˜ div")

            # æŸ¥æ‰¾å…ƒæ•°æ® divï¼ˆåŒ…å«æ–‡ä»¶æ‰©å±•åç­‰ä¿¡æ¯ï¼‰
            metadata_div = soup.find('div', class_=lambda x: x and 'text-gray-800' in x and 'font-semibold' in x and 'text-sm' in x and 'mt-4' in x)

            if metadata_div:
                # è·å–æ–‡æœ¬å¹¶ç”¨ä¸­é—´ç‚¹ï¼ˆÂ·ï¼‰åˆ†å‰²
                metadata_text = metadata_div.get_text(separator=' ', strip=True)
                parts = [part.strip() for part in metadata_text.split('Â·')]

                # æŸ¥æ‰¾åŒ¹é…åˆæ³•æ–‡ä»¶æ‰©å±•åçš„éƒ¨åˆ†
                for part in parts:
                    part_upper = part.upper()
                    for legal_ext in LEGAL_FILES:
                        # æ£€æŸ¥è¿™ä¸ªéƒ¨åˆ†æ˜¯å¦æ˜¯æ‰©å±•åï¼ˆå¦‚ "PDF"ã€"EPUB"ï¼‰
                        if part_upper == legal_ext.upper().replace('.', ''):
                            extension = legal_ext
                            d.logger.info(f"ä»å…ƒæ•°æ®ä¸­æå–æ‰©å±•å: {extension}")
                            break
                    if extension:
                        break

            # æ„å»ºæ–‡ä»¶å
            if title and extension:
                # æ¸…ç†æ ‡é¢˜ä¸­çš„éæ³•æ–‡ä»¶åå­—ç¬¦
                title = re.sub(r'[<>:"/\\|?*]', '_', title)
                # ç§»é™¤æœ«å°¾çš„å¥ç‚¹å’Œç©ºæ ¼ï¼Œé¿å…åŒé‡æ‰©å±•åï¼ˆå¦‚ "title..pdf"ï¼‰
                title = title.rstrip('. ')
                return f"{title}{extension}"
            elif title:
                # æ²¡æœ‰æ‰¾åˆ°æ‰©å±•åï¼Œåªä½¿ç”¨æ ‡é¢˜
                d.logger.warning("æœªèƒ½ä»å…ƒæ•°æ®ä¸­æå–æ–‡ä»¶æ‰©å±•å")
                return title
            else:
                # æ²¡æœ‰æ‰¾åˆ°æ ‡é¢˜
                return None

        # ==================== æ­¥éª¤ 1: æå–æ–‡ä»¶å ====================
        filename = None
        # æ ¹æ®ç”¨æˆ·åå¥½é€‰æ‹©æå–æ–¹æ³•
        if d.prefer_title_naming:
            # ä¼˜å…ˆä½¿ç”¨åŸºäºæ ‡é¢˜çš„å‘½åæ–¹å¼
            d.logger.info("ä½¿ç”¨åŸºäºæ ‡é¢˜çš„æ–‡ä»¶åæå–ï¼ˆé¦–é€‰ï¼‰")
            filename = extract_from_title()
            if not filename or filename == "Unknown":
                d.logger.warning("æ ‡é¢˜æå–å¤±è´¥ï¼Œå›é€€åˆ° Filepath å…ƒæ•°æ®")
                filename = extract_from_filepath()
        else:
            # ä¼˜å…ˆä½¿ç”¨ Filepath å…ƒæ•°æ®ï¼ˆé»˜è®¤ï¼‰
            filename = extract_from_filepath()
            if not filename:
                d.logger.warning("æœªæ‰¾åˆ° Filepath å…ƒæ•°æ®ï¼Œå›é€€åˆ°æ ‡é¢˜æå–")
                filename = extract_from_title()

        # æœ€ç»ˆå›é€€æ–¹æ¡ˆ - åœ¨æ–‡ä»¶åä¸­ä½¿ç”¨ MD5 å“ˆå¸Œå€¼
        if not filename:
            d.logger.warning("æœªæ‰¾åˆ°æ–‡ä»¶åï¼Œå›é€€åˆ° Unknown")
            filename = f"Unknown ({md5})"
        elif d.include_hash == "prefix":
            # åœ¨æ–‡ä»¶åå‰æ·»åŠ  MD5
            filename = f"{md5} - {filename}"
        elif d.include_hash == "suffix":
            # åœ¨æ–‡ä»¶ååæ·»åŠ  MD5
            filename = f"{filename} - {md5}"

        # ==================== æ­¥éª¤ 2: æ”¶é›†ä¸‹è½½é“¾æ¥ ====================
        links = []
        
        # æŸ¥æ‰¾ä¸‹è½½é¢æ¿ div
        # ä¸‹è½½é¢æ¿åŒ…å«æ‰€æœ‰çš„ä¸‹è½½é“¾æ¥
        downloads_panel = soup.find('div', id='md5-panel-downloads')
        if not downloads_panel:
            d.logger.warning("é¡µé¢ä¸Šæœªæ‰¾åˆ°ä¸‹è½½é¢æ¿")
            return filename, links
        
        # ==================== æ”¶é›†æ…¢é€Ÿä¸‹è½½é“¾æ¥ ====================
        # åªæ¥å— "no waitlist"ï¼ˆæ— ç­‰å¾…ï¼‰çš„æœåŠ¡å™¨
        for li in downloads_panel.find_all('li', class_='list-disc'):
            a = li.find('a', href=True)
            if not a:
                continue
            
            href = a['href']
            li_text = li.get_text().strip()
            
            # è·³è¿‡å¿«é€Ÿä¸‹è½½é“¾æ¥ï¼ˆæˆ‘ä»¬é€šè¿‡ API å¤„ç†ï¼‰
            if '/fast_download/' in href:
                continue
            
            # åªæ¥å—æ…¢é€Ÿä¸‹è½½é“¾æ¥
            if '/slow_download/' in href:
                # è·³è¿‡ç­‰å¾…é˜Ÿåˆ—æœåŠ¡å™¨ï¼ˆå®ƒä»¬æœ‰ 60 ç§’çš„ JavaScript å€’è®¡æ—¶ï¼‰
                if 'slightly faster but with waitlist' in li_text.lower():
                    d.logger.debug(f"è·³è¿‡ç­‰å¾…é˜Ÿåˆ—æœåŠ¡å™¨: {a.get_text().strip()}")
                    continue
                
                # æ¥å—æ— ç­‰å¾…æœåŠ¡å™¨
                if 'no waitlist' in li_text.lower():
                    # å°†ç›¸å¯¹ URL è½¬æ¢ä¸ºç»å¯¹ URL
                    full_url = urljoin(url, href)
                    server_name = a.get_text().strip() or "Slow Partner Server"
                    
                    links.append({
                        'url': full_url,
                        'domain': 'annas-archive.org',
                        'text': server_name,
                        'type': 'slow_download'
                    })
                    d.logger.debug(f"æ·»åŠ æ— ç­‰å¾…æœåŠ¡å™¨: {server_name}")
        
        # ==================== æ”¶é›†å¤–éƒ¨é•œåƒé“¾æ¥ ====================
        # åœ¨ js-show-external ul ä¸­æŸ¥æ‰¾
        external_ul = downloads_panel.find('ul', class_='js-show-external')
        if external_ul:
            for a in external_ul.find_all('a', href=True):
                href = a['href']

                # åªæ·»åŠ å®Œæ•´çš„ URL
                if not href.startswith('http'):
                    continue

                # è·³è¿‡ .onion URLï¼ˆTor æš—ç½‘é“¾æ¥ï¼‰
                if '.onion' in href.lower():
                    d.logger.debug(f"è·³è¿‡ .onion URL: {href}")
                    continue

                # è§£æ URL è·å–åŸŸå
                parsed = urlparse(href)
                domain = parsed.netloc

                # å¦‚æœæ²¡æœ‰æœ‰æ•ˆåŸŸåï¼Œè·³è¿‡
                if not domain:
                    continue

                links.append({
                    'url': href,
                    'domain': domain,
                    'text': domain,
                    'type': 'external_mirror'
                })
                d.logger.debug(f"æ·»åŠ å¤–éƒ¨é•œåƒ: {domain}")

        return filename, links

    except Exception as e:
        d.logger.error(f"è·å–ä¸‹è½½é“¾æ¥æ—¶å‡ºé”™: {e}")
        return "Unknown", []